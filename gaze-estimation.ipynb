{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaze Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "#Basic\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "#Aiding conversions from string to list\n",
    "import ast\n",
    "\n",
    "#Randomize\n",
    "import random\n",
    "\n",
    "#Labelling classifiers\n",
    "import time\n",
    "\n",
    "#Support Vector Classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from joblib import dump, load\n",
    "#from sklearn import svm, grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recording a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just record it. Falar de algumas precauções e cuidades. IDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Converting the video in measurable data / Data treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing the text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we created [datasetGenerating.py](https://github.com/antonioramiro/gaze-estimation/blob/master/datasetGenerating.py) which converts a video into an array of (~) 47 elements. yada yada yada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``demo da datasetGenerating`` .gif e o comando q se usa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to cluster every vector generated, each in an individual .txt file, we used [txtJoiner](https://github.com/antonioramiro/gaze-estimation/blob/master/txtJoiner.py), which outputs the following file yada yada. When said file is imported, the lines are read as strings, therefore it needs to be converted to a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the file\n",
    "with open('dataset_2020-03-24.txt') as file:\n",
    "    txt_file = [line.strip() for line in file]\n",
    "\n",
    "dataset = []\n",
    "#converting a list of strings to a list of lists\n",
    "counter = 0\n",
    "total_lines = len(txt_file)\n",
    "while counter != total_lines:\n",
    "    individual_line = ast.literal_eval(txt_file[counter])\n",
    "    dataset += [individual_line]\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meter aqui imagem do boneco, com a legenda adequada ao nosso vetor. como por aquilo bonito? fica melhor explicado por escrito do que tudo discriminado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``[x_nose, y_nose, x_right ear, y_right ear, x_left ear, y_left ear, x_right eye, y_right eye, x_left eye, y_left eye, x_right hand, y_right hand, x_left hand, y_left hand, ... contextual information - 32 positions ..., quadrant]``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Increasing sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inverter horizontalmente as cenas + termos o dobro dos dados. nota: a geradora do dataset cospe a resolução das imagens na sua penúltima linha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially, the dataset is composed by 11 elements.\n"
     ]
    }
   ],
   "source": [
    "print('Initially, the dataset is composed by ' + str(len(dataset)) + ' elements.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = (640,352) \n",
    "flipped_dataset = []\n",
    "\n",
    "# Calculate the simetric quadrant, where the input is an int and the output is a list of 1 element.\n",
    "# Given the numeration of quadrants (stated in 2.1.1), a simple addition/subtraction of 3 or 5 will yield\n",
    "# the simetric quadrant\n",
    "\n",
    "def flipped_quadrant(quadrant):\n",
    "    if quadrant in [1,3,9,11]:\n",
    "        flipped_quadrant = quadrant + 3 \n",
    "    elif quadrant in [4,6,12,14]:\n",
    "        flipped_quadrant = quadrant - 3 \n",
    "    elif quadrant in [0,2,8,10]:\n",
    "        flipped_quadrant = quadrant + 5\n",
    "    elif quadrant in [5,7,13,15]:\n",
    "        flipped_quadrant = quadrant - 5  \n",
    "    return [flipped_quadrant]\n",
    "    \n",
    "\n",
    "for element in dataset:\n",
    "    flipped_element = []\n",
    "    for i in range(len(element) - 33):\n",
    "        \n",
    "        # Every even index (up to the 14th: indexes that correspond to coordinates of poseKeypoints) contains \n",
    "        # an X coordinate, from which can be obtained the simetric coordinate (by subtracting to the width\n",
    "        # the original X), note that Y is irrelevant to horizontal simetry.\n",
    "        \n",
    "        if i%2 == 0 and element[i] != -1: #x\n",
    "            flipped_element += [resolution[0] - element[i]]\n",
    "                                            \n",
    "        else: #y or x == -1\n",
    "            flipped_element += [element[i]]       \n",
    "        \n",
    "    flipped_dataset += [flipped_element + element[14:-1] + flipped_quadrant(element[-1])]\n",
    "\n",
    "dataset += flipped_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, it has 22 elements.\n"
     ]
    }
   ],
   "source": [
    "print('Now, it has ' + str(len(dataset)) + ' elements.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Leveling the data - corrigir numeracao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em vez de leveling, outra palavra para descrever este processo de fazer os numeros mais proximos.  Due to XXXX, some quadrants tend to be more prevalent. So in order not to confuse :p the SVM, the values should be more homogenized  Falar um pouco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly lets visualise the distributtion of data with the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(data):\n",
    "    return 0\n",
    "\n",
    "# https://stackoverflow.com/a/29528483/11606537\n",
    "\n",
    "# faz com que dê output a uma imagem como essa e que também dê dados para serem usados na função que faz \n",
    "# o levelling propriamente dito. ser o maximo relativa possivel, nomeadamente o valor que queremos que \n",
    "# cada quadrante tenha de exemplos. Usa cenas do panda\n",
    "# para aceder ao quadrante, dataset[i][-1], se nao me engano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to leveling, we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall define the levelling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levelling(a):\n",
    "    return 0\n",
    "\n",
    "#faz com que printe, no final, considerações sobre o efectuado, tipo foram ignorados um total de XXX frames (? % do\n",
    "# dataset original) neste processo, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And apply it to our data, seeing a more homogeneyus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levelling(0)\n",
    "heatmap(0)\n",
    "\n",
    "#guardar o novo dataset em dataset, var global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Separating data for testing and for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the party going. Separate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames = len(dataset)\n",
    "testing_data = []\n",
    "training_data = []\n",
    "\n",
    "for i in range(int(0.2*total_frames)):\n",
    "    element_to_transfer = random.choice(dataset)\n",
    "    testing_data += [element_to_transfer]\n",
    "    dataset.remove(element_to_transfer)\n",
    "\n",
    "training_data = dataset\n",
    "dataset = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 0.18181818181818182), (18, 0.8181818181818182))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(testing_data),len(testing_data)/size),(len(training_data),len(training_data)/size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the models & Classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating a proper list of vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este step is only done here in order to reduce the probability of messing up and disconecting each X vector from the correspondant Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eXifY(data):\n",
    "    X, Y = [element[0:-1] for element in data], [element[-1] for element in data]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eXifY(training_data)[0]) == len(eXifY(training_data)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training & Classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o obj. é comparar a qualidade das classificações consoante o tipo de contxt dado, vamos treinar 4 modelos e testá-los e ver as diferenças, assim ta pa treinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-ad63199ebc2b>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-ad63199ebc2b>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    def coordsandquadrvectors(linestxt)\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def trainning(X,y):\n",
    "    \n",
    "    #tudo o que está ali abaixo, depois entrava aqui\n",
    "    \n",
    "    tipo = (exemplo) # exemplo mal feito para depois identificares no nome qual o tipo de dados\n",
    "        if len(X) = 46 (tudo) #nao vi bem os nrs\n",
    "        if len(X) = 10 (cabeca)\n",
    "        if len(X) = 14 (cabeca e maos)\n",
    "        if len(X) = 43 (cabeca e contexto)\n",
    "    return 'classificador com data e tipo^ de dados no nome.joblib','info do classif,+ +tipo + n da amostra.txt' \n",
    "\n",
    "\n",
    "#Opening the traindata file with all the coordinates of the important points.\n",
    "#The last number of each line is the number of the quadrant \n",
    "with open(\"traindata.txt\") as file: #ATTENTION: this filename comes from elsewhere, change if needed\n",
    "    linestrain = [line.strip() for line in file] #erases the empty spaces in each line from the txt file\n",
    "\n",
    "    \n",
    "def coordsandquadrvectors(linestxt)\n",
    "\n",
    "#Initialization  \n",
    "numberquad = [] \n",
    "Allthecoords = [] \n",
    "counter = 0\n",
    "\n",
    "length = len(linestxt)\n",
    "\n",
    "#This cicle separates, in each line corresponding to one image\n",
    "#the coordinates to the Allthecoords vector and the quadrant number to the numberquad vector\n",
    "while counter != length:\n",
    "    omega = ast.literal_eval(linestxt[counter]) # ast.literal_eval raises an exception if the input isn't a valid Python datatype, so the code won't be executed if it's not\n",
    "    Allthecoords += [omega[:10]]\n",
    "    if omega[-1] == -1: #if the quadrant number is -1 (mistakenly) it is replaced by the quadrant 0\n",
    "        numberquad +=[0]\n",
    "    else:\n",
    "        numberquad +=[omega[-1]]\n",
    "    counter+=1\n",
    "\n",
    "return Allthecoords, numberquad, length\n",
    "\n",
    "CoordsTrain, quadTrain = coordsandquadrvectors(linestrain)\n",
    "    \n",
    "#This function below selects the best parameters that will there be used with the SVC\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [1000,100] #parameter 1\n",
    "    gammas = [1e-06] #parameter 2  \n",
    "    param_grid = {'C': Cs, 'gamma' : gammas} #all in one grid\n",
    "    grid_search2 = grid_search.GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search2.fit(X, y)\n",
    "    grid_search2.best_params_\n",
    "    return grid_search2.best_params_\n",
    "    \n",
    "best = svc_param_selection(CoordsTrain, quadTrain, 3) #3 means a 3-fold cross-validation\n",
    "\n",
    "clf = SVC(C= best['C'],gamma=best['gamma']) #creating the model with the best parameters\n",
    "clf.fit(CoordsTrain, quadTrain) #fitting the model to the data\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp = str(now.strftime(\"%Y%m%d_%H-%M-%S\")) #the current day and time to have to the classifier name \n",
    "\n",
    "classif_name = \"classifier_\"+timestamp+\".joblib\" \n",
    "\n",
    "dump(clf,classif_name, protocol=2) #creating the classifier file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assim ta pa testar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(X,y,classif)\n",
    "\n",
    "    meter considerações osbre a eficácia deste classificador no .txt criado anteriormente\n",
    "    return conf matrixes e printar considerações\n",
    "    mete o classif em variavel global, para depois o poder puxar sem ter de dizer o nome do ficheiro\n",
    "\n",
    "\n",
    "\n",
    "#ATTENTION: the covariance matrices comments are TEMPORARY since we don't know if it's necessary (one method is implemented but not sure if it's the right one)\n",
    "\n",
    "#Opening the testdata file with all the coordinates of the important points.\n",
    "#The last number of each line is the number of the quadrant \n",
    "with open(\"testdata.txt\") as file: #ATTENTION: this filename comes from elsewhere, change if needed\n",
    "    linestest = [line.strip() for line in file]\n",
    "\n",
    "coordsTest,quadTest, lengthtest = coordsandquadrvectors(linestest)  \n",
    "\n",
    "#Predicting part: using the classifier to predict \n",
    "#(NOT USING THE FILE SAVED, IT ONLY RUNS IF THE CLASSIFIER WAS BUILT IN THE SAME SESSION)\n",
    "\n",
    "#Extra Initialization \n",
    "Qpredicted= []\n",
    "nmatching = 0 #number of right predictions\n",
    "confMatrix = np.zeros((16,16))\n",
    "i=0\n",
    "\n",
    "while i != lengthtest:\n",
    "Qpredicted+= [str(clf2.predict(np.array([coordsTest[i]]))).strip('['+']')] #predict\n",
    "quadTest[i]= str(quadTest[i]) #converting to a string\n",
    "    \n",
    "confMatrix[int(Qpredicted[i])][int(quadTest[i])] += 1 \n",
    "    \n",
    "if Qpredicted[i] == quadTest[i]:\n",
    "    nmatching+=1\n",
    "    \n",
    "i+=1\n",
    "\n",
    "persucess = (float(nmatching)/float(lengthtest)) * 100  \n",
    "\n",
    "\n",
    "print('The percentage of success was: ' + str(persucess)+ ' %')\n",
    "\n",
    "#print(str(confMatrix))\n",
    "\n",
    "#def sum1(input):\n",
    " #   sum = 0\n",
    "  #  for row in range (len(input)):\n",
    "   #     for col in range(len(input[0])):\n",
    "    #        sum = sum + input[row][col]\n",
    "\n",
    "#    return sum\n",
    "#print(sum1(confMatrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Using only the face keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using only the keypoints of the face, we'll delete context info (short/long) and hands coordinates. As coords da cara são os 10 primeiros items 5*2 (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = eXifY(training_data)\n",
    "X = [i[:10] for i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 6, 13, 13, 6, 6, 6, 12, 6, 3, 2, 3, 3, 3, 3, 8, 9, 3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dizer coisas aqui sobre o que se vai passar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainning(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now ... Dizer coisas aqui sobre o que se vai passar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = eXifY(testing_data)\n",
    "X = [i[:10] for i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Using only keypoints (face + hands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using only the keypoints, we'll delete context info (short/long). os 14 primeiros items 5*2 (x,y) da cara  + 2*2 (x,y) das mãos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = eXifY(training_data)\n",
    "X = [i[:12] for i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainning(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = eXifY(testing_data)\n",
    "X = [i[:12] for i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Using only the keypoints of the face and context data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using only the keypoints, we'll delete hands coordinates. #primeiros 10 + do 15 até ao 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = eXifY(training_data)\n",
    "X = [i[:10] + i[14:] for i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainning(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = eXifY(testing_data)\n",
    "X = [i[:10] + i[14:] for i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Using they keypoints of the face, hands and context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using only the keypoints, we'll delete context info (short/long) and hands coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = eXifY(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = eXifY(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fazer conf. matrix bonita https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
